{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LeonardNgo\\miniconda3\\envs\\cvmpa\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.ops import FeaturePyramidNetwork, MLP, sigmoid_focal_loss\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.io import savemat\n",
    "\n",
    "import reader\n",
    "import utils\n",
    "import network\n",
    "import transformer\n",
    "import cvmpca_2 as cvmpca\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "from my_trainer import SetCriterion\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo'\n",
    "# training_dir = './data/train'\n",
    "# testing_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_loader = reader.UnityDataset.from_unity_to_loader(root=train_folder, batch_size=2)\n",
    "\n",
    "# for sample in train_loader.dataset:\n",
    "#     pass\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "\n",
    "    # for key, image in image_dicts.items():\n",
    "    #     print(key, image.shape)\n",
    "    # print('-'*30)\n",
    "    # for targets in object_list:\n",
    "    # print(object_list.position.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "\n",
    "    # for key, image in image_dicts.items():\n",
    "    #     print(key, image.shape)\n",
    "    # print('-'*30)\n",
    "    # for targets in object_list:\n",
    "    for objs in object_list:\n",
    "        print(objs.category.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures = train_loader.dataset.captures\n",
    "cameras = {capture.id: utils.Camera.from_unity(capture) for capture in captures}\n",
    "for key, item in image_dicts.items():\n",
    "    print(key, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = transformer.Swin(is_trainable=True)\n",
    "fpn = FeaturePyramidNetwork(swin.embed_dims, swin.embed_dim)\n",
    "embed_dim = swin.embed_dim\n",
    "embed_dims = swin.embed_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(image_dicts.keys())  # camera ids\n",
    "B = image_dicts[keys[0]].size(0)\n",
    "\n",
    "visual_features = {\n",
    "    camera_key: fpn({\n",
    "        f'feat{i}': x.permute(0, 3, 1, 2)\n",
    "        for i, x in enumerate(swin(images))\n",
    "    })\n",
    "    for camera_key, images in image_dicts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, items in visual_features.items():\n",
    "    for key_, item in items.items():\n",
    "        print(key, key_, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(h, w):\n",
    "    x = torch.arange(0, w, dtype=torch.float)\n",
    "    y = torch.arange(0, h, dtype=torch.float)\n",
    "    grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "    return torch.stack([grid_x.flatten(), grid_y.flatten(), torch.ones(h*w)], dim=-1)\n",
    "\n",
    "cnn_layer = nn.Conv2d(3, 96, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "for key, items in visual_features.items():\n",
    "    print(key)\n",
    "    for key_, item in items.items():\n",
    "        B, D, H, W = item.shape\n",
    "        grid = create_grid(H, W)#.unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "        print(grid.shape)\n",
    "        rays = cameras[key].pix2ray(grid, [H, W, 1])\n",
    "        rays = rays.reshape(H, W, 3).permute(2, 0, 1).unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "        print(rays.shape)\n",
    "        # rays = rays.unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "        # print(grid.shape)\n",
    "        print(cnn_layer(rays).shape)\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _voxelize(space, voxel_size):\n",
    "    X = torch.arange(space[0][0], space[0][1] + voxel_size[0]/8, voxel_size[0])\n",
    "    Y = torch.arange(space[1][0], space[1][1] + voxel_size[1]/8, voxel_size[1])\n",
    "    Z = torch.arange(space[2][0], space[2][1] + voxel_size[2]/8, voxel_size[2])\n",
    "    # print(X.shape, Y.shape, Z.shape)\n",
    "\n",
    "    grid_x, grid_y, grid_z = torch.meshgrid(X, Y, Z, indexing='ij')\n",
    "    return torch.stack([grid_x.flatten(), grid_y.flatten(), grid_z.flatten()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class, k = 2, 6\n",
    "# layers = nn.ModuleList([\n",
    "#     cvmpca.VoxelMHA(\n",
    "#         embed_dim=embed_dim, num_heads=12, attention_dropout=0.1, dropout=0.1,\n",
    "#         cameras=cameras\n",
    "#     ) for _ in range(len(swin.embed_dims))\n",
    "# ])\n",
    "cls_embedding = nn.Embedding(n_class, embed_dim)\n",
    "\n",
    "space = torch.tensor([[-11, 11], [0, 3], [-7, 7]])\n",
    "voxel_size = [0.1, 0.2, 0.1]\n",
    "\n",
    "def _original(indices, n_bins):\n",
    "    X, Y, Z = n_bins\n",
    "\n",
    "    x, y, z = (indices//Z)//Y, (indices//Z)%Y, indices%Z\n",
    "    \n",
    "    return torch.stack([x, y, z], dim=-1)\n",
    "\n",
    "head = nn.Linear(embed_dim, n_class, bias=False)\n",
    "\n",
    "next_voxel_size = np.array(voxel_size)/(3**0)\n",
    "\n",
    "voxels = _voxelize(space, next_voxel_size)\n",
    "voxels = voxels.unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "camera_features = {}\n",
    "for camera_key, feature_dict in visual_features.items():\n",
    "    camera = cameras[camera_key]\n",
    "    feature = list(feature_dict.values())[-1]\n",
    "\n",
    "    *_, H, W = feature.shape\n",
    "    pixel_coor = camera(voxels, [H, W, 1])  # (B, N, 2)\n",
    "\n",
    "    bounding = torch.logical_and(\n",
    "        torch.logical_and(pixel_coor[..., 0] >= 0, pixel_coor[..., 0] <= H),\n",
    "        torch.logical_and(pixel_coor[..., 1] >= 0, pixel_coor[..., 1] <= W)\n",
    "    )  # (B, N)\n",
    "\n",
    "    # for grid_sample compatibility. (B, N, 1, 2)\n",
    "    pixel_coor = pixel_coor.unsqueeze(-2)\n",
    "    camera_feature = F.grid_sample(feature, pixel_coor, align_corners=True).squeeze(-1)\n",
    "    camera_feature = camera_feature*bounding.unsqueeze(1)#.unsqueeze(0)  # (B, D, N)\n",
    "    camera_feature = camera_feature.permute(0, 2, 1)  # (B, N, D)\n",
    "    \n",
    "    camera_features[camera_key] = camera_feature\n",
    "    # camera_feature = camera_feature.reshape(B, C, K, N, -1)\n",
    "\n",
    "for key, item in camera_features.items():\n",
    "    print(key, item.shape)\n",
    "\n",
    "voxel_features = torch.stack([item for item in camera_features.values()], dim=-1).sum(-1)\n",
    "print(voxel_features.shape)\n",
    "\n",
    "logits = head(voxel_features)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = logits.view(B, -1).topk(k, dim=-1)\n",
    "top_proposals_index = indices // n_class  # (B, K) which voxel\n",
    "top_proposals_class = indices % n_class   # (B, K) which class\n",
    "top_proposals_index.shape, top_proposals_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_voxels = voxel_features.gather(1, top_proposals_index.unsqueeze(-1).repeat(1, 1, embed_dim))\n",
    "selected_voxels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(voxel_features[torch.arange(1).unsqueeze(-1), top_proposals_index] == selected_voxels).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cvmpca.CVMPCA(\n",
    "    train_loader.dataset.captures,\n",
    "    n_classes=len(train_loader.dataset.category_lookup)-1,  # ignore background\n",
    "    space=[[-11, 11], [0, 3], [-7, 7]],\n",
    "    voxel_size=[0.2, 0.3, 0.2],\n",
    "    ratio=3,\n",
    ")\n",
    "# outputs = net(50, image_dicts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dicts, object_list = batch\n",
    "output_list, points = net(50, image_dicts)\n",
    "# net.cuda()\n",
    "# output_list, voxel_list = net(50, {key: item.cuda() for key, item in image_dicts.items()})\n",
    "# B, L, C, P, _ = cls_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = net._match(\n",
    "    (output_list, points),\n",
    "    object_list\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0 | cameras | ModuleDict            | 0     \n",
      "1 | swin    | Swin                  | 27.6 M\n",
      "2 | fpn     | FeaturePyramidNetwork | 470 K \n",
      "3 | ray_pe  | Conv2d                | 384   \n",
      "4 | blocks  | ModuleList            | 513 K \n",
      "5 | heads   | ModuleList            | 99.4 K\n",
      "--------------------------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.661   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a859fe868b8246e9aed6bef9ac0a6115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=100, #precision='16-mixed',\n",
    "    gradient_clip_val=35,\n",
    "    log_every_n_steps=20,\n",
    "    # accelerator=\"cpu\"\n",
    "    # profiler=\"simple\",\n",
    "    # detect_anomaly=True\n",
    ")\n",
    "net = cvmpca.CVMPCA(\n",
    "    train_loader.dataset.captures,\n",
    "    n_classes=len(train_loader.dataset.category_lookup)-1,\n",
    "    space=[[-11, 11], [0, 3], [-7, 7]],\n",
    "    voxel_size=[0.075, 0.2, 0.075],\n",
    "    ratio=3,\n",
    ")\n",
    "# loss_func = SetCriterion(\n",
    "#     num_ue=50, num_sbs=2, num_class=1,\n",
    "#     num_layers=3, pc_range=[[-5, 5], [0, 3], [-5, 5]]\n",
    "# )\n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.space[:, 0], net.space[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset.category_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = cvmpca.CVMPCA(\n",
    "    train_loader.dataset.captures,\n",
    "    n_classes=len(train_loader.dataset.category_lookup),\n",
    "    space=[[-11, 11], [0, 3], [-7, 7]],\n",
    "    # voxel_size=[0.075, 0.2, 0.075],\n",
    "    voxel_size=[0.2, .3, .2],\n",
    "    ratio=3,\n",
    ").cuda()\n",
    "\n",
    "\n",
    "def _get_src_permutation_idx(indices):\n",
    "    # permute predictions following indices\n",
    "    batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "    src_idx = torch.cat([src for (src, _) in indices])\n",
    "    return batch_idx, src_idx\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "    for obj in object_list:\n",
    "        obj.category = obj.category.cuda()\n",
    "        obj.position = obj.position.cuda()\n",
    "        obj.los = obj.los.cuda()\n",
    "    image_dicts = {\n",
    "        key: item.cuda()\n",
    "        for key, item in image_dicts.items()\n",
    "    }\n",
    "    break\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "for i in range(1000):\n",
    "    output_dict = net(200, image_dicts)\n",
    "    # top_indices = output_dict['top_indices']\n",
    "\n",
    "    index_list = net._match(output_dict['output_list'], object_list)\n",
    "    # perform matching\n",
    "    # with torch.no_grad():\n",
    "    #     # B, K = top_indices.shape\n",
    "    #     # print(output_dict['voxels'].shape)\n",
    "    #     B, N = output_dict['output_list'][0]['position'].shape[:2]\n",
    "\n",
    "    #     tgt_ids = torch.cat([tgt.category for tgt in object_list]).argmax(-1)\n",
    "    #     tgt_pos = torch.cat([tgt.position for tgt in object_list])\n",
    "    #     tgt_link = torch.cat([tgt.los for tgt in object_list]).sigmoid()\n",
    "\n",
    "    #     # b_ids = torch.arange(B)\n",
    "    #     out_prob = output_dict['output_list'][0]['classification'].flatten(0, 1).softmax(-1)\n",
    "    #     cost_class = -out_prob[:, tgt_ids]\n",
    "    #     # print(out_prob.shape, cost_class.shape)\n",
    "        \n",
    "    #     out_pos = output_dict['output_list'][0]['position'].flatten(0, 1)\n",
    "    #     # print(out_pos, tgt_pos)\n",
    "    #     cost_pos = torch.cdist(out_pos, tgt_pos, p=1)\n",
    "    #     C = 0*cost_class + cost_pos\n",
    "    #     # print(cost_class.shape, cost_pos.shape, C.shape)\n",
    "    #     C = C.view(B, N, -1).cpu()\n",
    "\n",
    "    #     sizes = [len(obj.category) for obj in object_list]\n",
    "\n",
    "    #     # perform matching for each output-label pair in a batch\n",
    "    #     indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "    #     indices = [\n",
    "    #         (torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))\n",
    "    #         for i, j in indices\n",
    "    #     ]\n",
    "\n",
    "    target_class = torch.cat([obj.category[i] for obj, (_, i) in zip(object_list, indices)], dim=0)#.argmax(-1)\n",
    "    target_pos = torch.cat([obj.position[i] for obj, (_, i) in zip(object_list, indices)], dim=0)\n",
    "\n",
    "    idx = _get_src_permutation_idx(index_list[0])\n",
    "    # pred_class = output_dict['voxel_logits'][idx]\n",
    "    # pred_pos = output_dict['voxels'][idx]\n",
    "    pred_class = output_dict['output_list'][0]['classification'][idx]\n",
    "    pred_pos = output_dict['output_list'][0]['position'][idx]\n",
    "    \n",
    "    target_classes = torch.full_like(output_dict['output_list'][0]['classification'], 0.)\n",
    "    target_classes[idx] = target_class\n",
    "    ce_loss = sigmoid_focal_loss(output_dict['output_list'][0]['classification'], target_classes, reduction='mean')\n",
    "\n",
    "    pos_loss = F.l1_loss(pred_pos, target_pos, reduction='mean')\n",
    "    loss = ce_loss + pos_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'{i} {loss.item():.4f} {pos_loss.item():.4f}')\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict['output_list'][0]['position'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = net(200, image_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = net._match(output_dict['output_list'], object_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.training_step(batch, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
