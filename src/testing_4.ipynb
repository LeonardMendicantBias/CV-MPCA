{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LeonardNgo\\miniconda3\\envs\\cvmpa\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.ops import FeaturePyramidNetwork, MLP, sigmoid_focal_loss\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.io import savemat\n",
    "\n",
    "import reader\n",
    "import utils\n",
    "import network\n",
    "import transformer\n",
    "import cvmpca\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "from my_trainer import SetCriterion\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo'\n",
    "# training_dir = './data/train'\n",
    "# testing_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "train_loader = reader.UnityDataset.from_unity_to_loader(root=train_folder, batch_size=4)\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "\n",
    "    # for key, image in image_dicts.items():\n",
    "    #     print(key, image.shape)\n",
    "    # print('-'*30)\n",
    "    # for targets in object_list:\n",
    "    print(object_list.position.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = transformer.Swin(is_trainable=True)\n",
    "fpn = FeaturePyramidNetwork(swin.embed_dims, swin.embed_dim)\n",
    "embed_dim = swin.embed_dim\n",
    "embed_dims = swin.embed_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_0\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera_2\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera_1\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# process the image in each view\n",
    "\n",
    "keys = list(image_dicts.keys())  # camera ids\n",
    "B = image_dicts[keys[0]].size(0)\n",
    "\n",
    "visual_features = {\n",
    "    camera_key: fpn({\n",
    "        f'feat{i}': x.permute(0, 3, 1, 2)\n",
    "        for i, x in enumerate(swin(images))\n",
    "    })\n",
    "    for camera_key, images in image_dicts.items()\n",
    "}\n",
    "for camera_key, features in visual_features.items():\n",
    "    print(camera_key)\n",
    "    for key, feature in features.items():\n",
    "        print(key, feature.shape)\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvmpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
