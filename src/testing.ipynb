{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.io import savemat\n",
    "\n",
    "import reader\n",
    "import network\n",
    "from my_trainer import SetCriterion\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo'\n",
    "# training_dir = './data/train'\n",
    "# testing_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = reader.UnityDataset.from_unity_to_loader(root=train_folder, batch_size=8)\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "\n",
    "    # for key, image in image_dicts.items():\n",
    "    #     print(key, image.shape)\n",
    "    # print('-'*30)\n",
    "    # for targets in object_list:\n",
    "    #     print(targets.position)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    capture_lookup=train_loader.dataset.capture_lookup,\n",
    "    num_classes=len(train_loader.dataset.category_lookup), \n",
    "    num_layers=4, num_query=200,\n",
    "    pc_range=[[-11, 7], [0, 3], [-7, 7]]\n",
    ")\n",
    "cat_list, position_list, los_list = net.forward(image_dicts)\n",
    "\n",
    "# num_layers, num_query = cat_list.shape[1], cat_list.shape[2]\n",
    "# index_tuple_list = []\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     # for each sample in the batch\n",
    "#     for label, (cat, pos, los) in zip(object_list, zip(cat_list, position_list, los_list)):\n",
    "#         cat, pos, los = cat[0], pos[0], los[0]  # use the same indices for all levels\n",
    "#         C = np.zeros((label.category.shape[0], cat.shape[0]))  # (K, K_hat)\n",
    "\n",
    "#         for i in range(label.category.shape[0]):\n",
    "#             # print(cat.shape, label.category[i].shape)\n",
    "#             cat_delta = F.binary_cross_entropy_with_logits(\n",
    "#                 cat, label.category[i].unsqueeze(0).expand(num_query, -1), reduction='none'\n",
    "#             ).mean(-1)\n",
    "#             pos_delta = F.mse_loss(\n",
    "#                 pos*(net.loss.pc_range[:, 1] - net.loss.pc_range[:, 0]) + net.loss.pc_range[:, 0],\n",
    "#                 label.position[i].unsqueeze(0).expand(num_query, -1), reduction='none'\n",
    "#             ).mean(-1)\n",
    "#             los_delta = F.binary_cross_entropy_with_logits(\n",
    "#                 los, label.los[i].unsqueeze(0).expand(num_query, -1), reduction='none'\n",
    "#             ).mean(-1)\n",
    "#             # print(cat_delta.shape, pos_delta.shape, los_delta.shape)\n",
    "\n",
    "#             C[i] = cat_delta + pos_delta + los_delta\n",
    "\n",
    "#         label_indices, cat_indices = linear_sum_assignment(C)\n",
    "#         index_tuple_list.append((label_indices, cat_indices))\n",
    "\n",
    "\n",
    "# cls_losses, reg_losses, los_losses = [], [], []\n",
    "# for i, indices in enumerate(index_tuple_list):\n",
    "#     label_indices, cat_indices = indices\n",
    "#     cls_loss = F.binary_cross_entropy_with_logits(\n",
    "#         cat_list[i, :, cat_indices],\n",
    "#         object_list[i].category[label_indices].unsqueeze(0).expand(6, -1, -1)\n",
    "#     )\n",
    "#     reg_loss = F.mse_loss(\n",
    "#         position_list[i, :, cat_indices],#*(net.loss.pc_range[:, 1] - net.loss.pc_range[:, 0]) + net.loss.pc_range[:, 0],\n",
    "#         object_list[i].position[label_indices].unsqueeze(0).expand(6, -1, -1)\n",
    "#     )\n",
    "#     los_loss = F.binary_cross_entropy_with_logits(\n",
    "#         los_list[i, :, cat_indices],\n",
    "#         object_list[i].los[label_indices].unsqueeze(0).expand(6, -1, -1)\n",
    "#     )\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "    # net(image_dicts)\n",
    "    loss = net.training_step(batch, 1)\n",
    "    # print(loss)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type         | Params\n",
      "---------------------------------------------\n",
      "0 | cameras     | ModuleDict   | 0     \n",
      "1 | single_view | SingleView   | 28.1 M\n",
      "2 | query       | Embedding    | 19.2 K\n",
      "3 | to_anchor   | Linear       | 291   \n",
      "4 | decoder     | Decoder      | 567 K \n",
      "5 | loss        | SetCriterion | 0     \n",
      "---------------------------------------------\n",
      "28.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.6 M    Total params\n",
      "114.554   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03fa2f90bdb43989bc24fef318edbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    \"D:/runs\", name=\"CVMPCA\"\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50, precision='16-mixed',\n",
    "    gradient_clip_val=35,\n",
    "    logger=logger,\n",
    "    # callbacks=[\n",
    "    #     LearningRateMonitor(logging_interval='step')\n",
    "    # ],\n",
    "    log_every_n_steps=20,\n",
    "    # profiler=\"simple\",\n",
    ")\n",
    "net = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    capture_lookup=train_loader.dataset.capture_lookup,\n",
    "    num_classes=len(train_loader.dataset.category_lookup), \n",
    "    num_layers=6, num_query=200,\n",
    "    pc_range=[[-110, 70], [0, 30], [-70, 70]]\n",
    ")\n",
    "# loss_func = SetCriterion(\n",
    "#     num_ue=50, num_sbs=2, num_class=1,\n",
    "#     num_layers=3, pc_range=[[-5, 5], [0, 3], [-5, 5]]\n",
    "# )\n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range_tensor = torch.tensor([[-11, 7], [0, 3], [-7, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range_tensor[:, 1] - pc_range_tensor[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores, positions, visibilities = net({key: image.cuda() for key, image in image_dicts.items()})\n",
    "gt_scores, gt_positions, gt_visibilities = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indinces = scores[:, -1, :, 0].argmax(dim=-1)\n",
    "positions = positions.cpu()\n",
    "# print(positions_)\n",
    "print(positions.shape)\n",
    "# gt_positions\n",
    "\n",
    "pc_range = torch.tensor([[-5, 5], [0, 3], [-5, 5]], dtype=torch.float)\n",
    "for i, idx in enumerate(indinces.cpu()):\n",
    "    pos = positions[i, -1, idx]\n",
    "    # print(positions.shape, pos.shape)\n",
    "    pos = pos*(pc_range[:, 1] - pc_range[:, 0]) + pc_range[:, 0]\n",
    "    print(pos.detach())\n",
    "    print(gt_positions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_view = network.SingleView()\n",
    "\n",
    "visual_features = {\n",
    "    key: single_view(image)\n",
    "    for key, image in image_dicts.items()\n",
    "}\n",
    "for features in visual_features.values():\n",
    "    for feature in features.values():\n",
    "        print(feature.shape)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = network.MyTransformer(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    num_classes=1, num_levels=4,\n",
    "    num_layers=4, num_query=50,\n",
    "    embed_dim=96, num_feature_levels=4,\n",
    ")\n",
    "loss_func = SetCriterion(\n",
    "    num_ue=5, num_sbs=2, num_category=1,\n",
    ")\n",
    "\n",
    "scores, positions, visibilities = head(visual_features)\n",
    "\n",
    "loss_func(\n",
    "    (scores, positions, visibilities),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=200, precision=16)\n",
    "net = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    space_size=[[-5, 5], [0, 3], [-5, 5]],\n",
    "    voxel_size=[1., 1., 1.],\n",
    "    num_neck_layers=1,\n",
    "    num_ue=5, num_category=1,\n",
    ")\n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_grid(space_size, voxel_size, device=None):\n",
    "    nBins = [math.floor((space[1] - space[0])//voxel)+1 for space, voxel in zip(space_size, voxel_size)]\n",
    "    \n",
    "    X = torch.linspace(space_size[0][0], space_size[0][1], nBins[0], device=device)\n",
    "    Y = torch.linspace(space_size[1][0], space_size[1][1], nBins[1], device=device)\n",
    "    Z = torch.linspace(space_size[2][0], space_size[2][1], nBins[2], device=device)\n",
    "    \n",
    "    gridx, gridy, gridz = torch.meshgrid(X, Y, Z)\n",
    "    gridx = gridx.contiguous().view(-1, 1)\n",
    "    gridy = gridy.contiguous().view(-1, 1)\n",
    "    gridz = gridz.contiguous().view(-1, 1)\n",
    "    grid = torch.cat([gridx, gridy, gridz], dim=1)\n",
    "    return grid\n",
    "\n",
    "grid = compute_grid(\n",
    "    space_size=[[-4, 4], [0, 3], [-4, 4]],\n",
    "    voxel_size=[1, 1, 1],\n",
    ")\n",
    "# grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = SetCriterion(\n",
    "    num_ue=5, num_sbs=2, num_category=1,\n",
    ")\n",
    "model = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    space_size=[[-5, 5], [0, 3], [-5, 5]],\n",
    "    voxel_size=[.2, .2, .2],\n",
    "    num_neck_layers=1,\n",
    "    num_ue=5, num_category=1,\n",
    ")\n",
    "for batch in train_loader:\n",
    "    image_dict, gt_ues = batch\n",
    "\n",
    "    pred = model(image_dict, 5)\n",
    "    # loss = loss_func(pred, gt_ues)\n",
    "\n",
    "    # print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    pred_ues, pred_indices = pred\n",
    "    score_list, pos_list, vis_list = gt_ues\n",
    "\n",
    "    B, num_ue = pred_ues.shape[:2]\n",
    "\n",
    "    indices = []\n",
    "    # for each sample in a batch\n",
    "    for p_ue, p_indices, _, gt_p, gt_v in zip(pred_ues, pred_indices, score_list, pos_list, vis_list):\n",
    "        p_anchor = p_indices\n",
    "        \n",
    "        scores = p_ue[:, 0].sigmoid()\n",
    "        positions = p_ue[:, 1:4]\n",
    "        visibilities = p_ue[:, 4:]\n",
    "        \n",
    "        cost_pos = torch.cdist(positions, gt_p, p=1)\n",
    "        cost_vis = []\n",
    "        for p_v in visibilities:\n",
    "            cost_vis.append(torch.tensor([\n",
    "                F.binary_cross_entropy_with_logits(p_v, t_v) for t_v in gt_v\n",
    "            ], device=pred_ues[-1].device))\n",
    "        cost_vis = torch.stack(cost_vis)\n",
    "\n",
    "        C = scores.unsqueeze(-1) + cost_pos + cost_vis\n",
    "        indices.append(linear_sum_assignment(C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    space_size=[[-5, 5], [0, 3], [-5, 5]],\n",
    "    voxel_size=[.75, .75, .75],\n",
    "    num_neck_layers=1,\n",
    "    num_category=1,\n",
    "    num_ue=5, \n",
    ")\n",
    "loss = SetCriterion(\n",
    "    num_ue=5, num_sbs=2, num_category=1,\n",
    ")\n",
    "print('grid', model.grid.shape)\n",
    "for batch in train_loader:\n",
    "    image_dict, _ = batch\n",
    "    hierachical_visual_features = {key: model.backbone(image) for key, image in image_dict.items()}\n",
    "    \n",
    "    last_feature = {key: features[-1] for key, features in hierachical_visual_features.items()}\n",
    "    xs = {}\n",
    "    for key, feature in last_feature.items():\n",
    "        B, H, W, D = feature.size()\n",
    "        # convert the 3D positions into pixel coordinate\n",
    "        coor = model.cameras[key](model.grid, [H, W, 1])\n",
    "        # record out-of-bound 3D positions\n",
    "        bounding_x = coor[:, 0] > H\n",
    "        bounding_y = coor[:, 1] > W\n",
    "        bounding = torch.logical_and(bounding_x, bounding_y)\n",
    "        # make sure the pixel coordinate stay within the image for the 'gather' api\n",
    "        coor[:, 0] = torch.clamp(coor[:, 0], 0, H - 1)\n",
    "        coor[:, 1] = torch.clamp(coor[:, 1], 0, W - 1)\n",
    "\n",
    "        N = coor.size(0)\n",
    "        flat_indices = coor[:, 0] * H + coor[:, 1]\n",
    "        selected_features = torch.gather(\n",
    "            feature.view(B, H * W, D), dim=1,\n",
    "            index=flat_indices.long().view(1, N, 1).expand(B, -1, D)\n",
    "        )\n",
    "        # replace out-of-bound position with zero (by multiplying with zero)\n",
    "        selected_features = selected_features * bounding.logical_not().unsqueeze(0).unsqueeze(-1)\n",
    "        xs[key] = selected_features\n",
    "\n",
    "    voxels = torch.stack([x for x in xs.values()], dim=1).mean(1)\n",
    "    print(voxels.shape)\n",
    "    proposal = model.proposal(voxels)\n",
    "    print(proposal.shape)\n",
    "\n",
    "    k = 5\n",
    "    top_proposal, top_indices = torch.topk(proposal[:, :, 0], k, dim=1, largest=True, sorted=True)\n",
    "    indices_x = top_indices // (model.nBins[0] * model.nBins[1])\n",
    "    indices_y = (top_indices % (model.nBins[0] * model.nBins[1])) // model.nBins[0]\n",
    "    indices_z = (top_indices % (model.nBins[0] * model.nBins[1])) % model.nBins[0]\n",
    "    indices = torch.stack([indices_x, indices_y, indices_z], dim=-1)\n",
    "    print(indices.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=200, precision=16)\n",
    "net = network.CVMPCA(\n",
    "    captures=train_loader.dataset.captures,\n",
    "    space_size=[[-5, 5], [0, 3], [-5, 5]],\n",
    "    voxel_size=[1., 1., 1.],\n",
    "    num_neck_layers=1,\n",
    "    num_ue=5, num_category=1,\n",
    ")\n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
