{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LeonardNgo\\miniconda3\\envs\\cvmpa\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo_3'\n",
    "# training_dir = './data/train'\n",
    "# testing_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.0193212469174924 [767.9947  220.16843] [769 220]\n",
      "2 1.0140845578686535 [551.      220.16843] [550 220]\n",
      "3 18.852831218877323 [551.          3.1737087] [550  22]\n",
      "[767 220]\n",
      "[551 220]\n",
      "[551   3]\n"
     ]
    }
   ],
   "source": [
    "solo = Solo(data_path=train_folder)\n",
    "\n",
    "\n",
    "def pos_to_pixel(capture, pos) -> np.ndarray:\n",
    "    r = R.from_quat(capture.rotation)\n",
    "    translation = np.array(capture.position)\n",
    "\n",
    "    camera_coordinates = r.inv().apply((pos - translation).T).T - translation\n",
    "    pixel_coordinates = intrinsic @ camera_coordinates\n",
    "    pixels = pixel_coordinates / pixel_coordinates[-1]\n",
    "    # print()\n",
    "    # return pixels[:2, 0].astype(np.int_)\n",
    "    return pixels#.astype(np.int_)[:2]\n",
    "\n",
    "\n",
    "for frame_idx, frame in enumerate(solo.frames()):\n",
    "    # print(f'\\r{frame_idx}/{len(solo.frames())}', end='')\n",
    "    \n",
    "    for i, capture in enumerate(frame.captures):\n",
    "        camera = utils.Camera.from_unity(capture)\n",
    "\n",
    "        resolution = np.array(capture.dimension)\n",
    "        intrinsic = np.array([capture.matrix]).reshape((3, 3))\n",
    "        # intrinsic /= intrinsic[-1,-1]\n",
    "        # intrinsic[1, 1] = -intrinsic[1, 1]\n",
    "\n",
    "        translation = np.array(capture.position)\n",
    "\n",
    "        gt_pixels = {}\n",
    "        bboxes = [anno for anno in capture.annotations if isinstance(anno, BoundingBox2DAnnotation)][0]\n",
    "        for bbox in bboxes.values:\n",
    "            pixel = np.array(bbox.origin) + np.array(bbox.dimension)/2\n",
    "            gt_pixels[bbox.instanceId] = pixel.astype(np.int_)\n",
    "            # print(bbox.instanceId, bbox.labelName)\n",
    "\n",
    "        # for key, item in gt_pixels.items():\n",
    "        #     print(key, item)\n",
    "\n",
    "        r = R.from_quat(capture.rotation)\n",
    "        pixels = {}\n",
    "        anno_3d = [anno for anno in capture.annotations if isinstance(anno, BoundingBox3DAnnotation)][0]\n",
    "        \n",
    "        # relative coordinate convertion is correct (has been checked)\n",
    "        poses = []\n",
    "        for anno in anno_3d.values:\n",
    "            t = np.array(anno.translation)\n",
    "            # World coordinate = rorated camera coordinate + translation\n",
    "            # P_w = R_w @ P_c + t_w\n",
    "            pos = np.array(r.apply(t.T).T + translation)\n",
    "            poses.append(pos)\n",
    "            # print(pos)\n",
    "        \n",
    "        posses_ = [pos_to_pixel(capture, pos) for pos in poses]\n",
    "        for idx, pos, gt_pos in zip([anno.instanceId for anno in anno_3d.values], camera(torch.tensor(poses), [*resolution, 1]), gt_pixels.values()):\n",
    "        # for idx, pos, gt_pos in zip([anno.instanceId for anno in anno_3d.values], posses_, gt_pixels.values()):\n",
    "            print(idx, np.sqrt(((pos.numpy() - gt_pos)**2).sum()), pos.numpy(), gt_pos)\n",
    "            # print(idx, pos, gt_pos)\n",
    "        \n",
    "        for world_pos in poses:\n",
    "            camera_pos = (world_pos - translation) @ r.inv().as_matrix()\n",
    "            # print(camera_pos)\n",
    "\n",
    "            # pixel_coordinates = camera_pos @ (intrinsic * (np.diag([-resolution[0], resolution[1], 1])/2)).T\n",
    "            pixel_coordinates = intrinsic.dot(camera_pos)\n",
    "            # pixel_coordinates = (intrinsic * (np.diag([resolution[0], resolution[1], 1])/2)).dot(camera_pos)\n",
    "            # print(pixel_coordinates)\n",
    "            if pixel_coordinates[-1] != 0:\n",
    "                pixel_coordinates = pixel_coordinates / pixel_coordinates[-1]\n",
    "            # print(pixel_coordinates)\n",
    "            # pixel = pixel_coordinates[:2] + np.array(resolution)/2\n",
    "            # print(pixel)\n",
    "            print(\n",
    "                np.array([\n",
    "                    int(-(pixel_coordinates[0] * resolution[0]) / 2.0 + (resolution[0] * 0.5)),\n",
    "                    int((pixel_coordinates[1] * resolution[1]) / 2.0 + (resolution[1] * 0.5)),\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        break\n",
    "        print('-'*30)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _project_pt_to_pixel_location(pt, projection, img_height, img_width):\n",
    "    _pt = projection.dot(pt)\n",
    "\n",
    "    # compute the perspective divide. Near clipping plane should take care of\n",
    "    # divide by zero cases, but we will check to be sure\n",
    "\n",
    "    if _pt[2] != 0:\n",
    "        _pt /= _pt[2]\n",
    "\n",
    "    return np.array([\n",
    "        int(-(_pt[0] * img_width) / 2.0 + (img_width * 0.5)),\n",
    "        int((_pt[1] * img_height) / 2.0 + (img_height * 0.5)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91891291, -0.        , -0.        ],\n",
       "       [-0.        ,  1.731012  , -0.        ],\n",
       "       [-0.        , -0.        ,  1.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -0.91891291,   -0.        ,   -0.        ],\n",
       "       [  -0.        ,   -1.731012  ,   -0.        ],\n",
       "       [-551.        ,  292.5       ,    1.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91891291, -0.        , -0.        ],\n",
       "       [-0.        , -1.731012  , -0.        ],\n",
       "       [-0.        , -0.        ,  1.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([155.00001243, -44.99997513, 179.99999927])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.identity().from_quat([\n",
    "    0.08282778, 0.901980042, 0.199964061, -0.373612136\n",
    "]).as_euler('xyz', degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08282785  0.9019799   0.19996413 -0.37361231]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([155., -45., 180.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quat = R.from_euler('xyz', [-25, 225, 0], degrees=True).as_quat()\n",
    "print(quat)\n",
    "R.from_quat(quat).as_euler('xyz', degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([155., -45., 180.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_euler('xyz', [-25, 225, 0], degrees=True).as_euler('xyz', degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        #     # camera_coordinates = r.inv().apply((pos - translation).T).T\n",
    "        #     ueler = r.as_matrix()\n",
    "        #     # print('pos', pos, pos - translation)\n",
    "        #     camera_coordinates = (pos - translation) @ ueler\n",
    "        #     # print(camera_coordinates)\n",
    "        #     _intrinsic = intrinsic * np.array([-resolution[0]/2, resolution[1]/2, 1])\n",
    "        #     # print(camera_coordinates.shape)\n",
    "        #     pixel_coordinates = camera_coordinates @ _intrinsic\n",
    "        #     # print(pixel_coordinates)\n",
    "        #     # print(camera_coordinates.shape, intrinsic.shape)\n",
    "        #     pixel = pixel_coordinates / pixel_coordinates[-1]\n",
    "        #     pixel[:2] += resolution/2\n",
    "        #     pixels[anno.instanceId] = pixel[:2].astype(np.int_)\n",
    "        # poses = np.array(poses)\n",
    "        # # print(poses.shape)\n",
    "        # camera_coordinates = (poses - translation[np.newaxis, ...]) @ ueler\n",
    "        # # print(camera_coordinates.shape)\n",
    "        # pixel_coordinates = camera_coordinates @ _intrinsic\n",
    "        # pixel = pixel_coordinates / pixel_coordinates[:, -1]\n",
    "        # pixel[:, :2] += resolution/2\n",
    "        # print(pixel[:, :2].astype(np.int_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
