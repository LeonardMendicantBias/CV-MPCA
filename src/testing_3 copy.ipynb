{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.ops import FeaturePyramidNetwork, MLP, sigmoid_focal_loss\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.io import savemat\n",
    "\n",
    "import reader\n",
    "import utils\n",
    "import network\n",
    "import transformer\n",
    "import cvmpca\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "from my_trainer import SetCriterion\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo'\n",
    "# training_dir = './data/train'\n",
    "# testing_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "train_loader = reader.UnityDataset.from_unity_to_loader(root=train_folder, batch_size=4)\n",
    "\n",
    "for batch in train_loader:\n",
    "    image_dicts, object_list = batch\n",
    "\n",
    "    # for key, image in image_dicts.items():\n",
    "    #     print(key, image.shape)\n",
    "    # print('-'*30)\n",
    "    # for targets in object_list:\n",
    "    print(object_list.position.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_0 torch.Size([4, 3, 256, 256])\n",
      "camera_2 torch.Size([4, 3, 256, 256])\n",
      "camera torch.Size([4, 3, 256, 256])\n",
      "camera_1 torch.Size([4, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "captures = train_loader.dataset.captures\n",
    "cameras = {capture.id: utils.Camera.from_unity(capture) for capture in captures}\n",
    "for key, item in image_dicts.items():\n",
    "    print(key, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = transformer.Swin(is_trainable=True)\n",
    "fpn = FeaturePyramidNetwork(swin.embed_dims, swin.embed_dim)\n",
    "embed_dim = swin.embed_dim\n",
    "embed_dims = swin.embed_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(image_dicts.keys())  # camera ids\n",
    "B = image_dicts[keys[0]].size(0)\n",
    "\n",
    "visual_features = {\n",
    "    camera_key: fpn({\n",
    "        f'feat{i}': x.permute(0, 3, 1, 2)\n",
    "        for i, x in enumerate(swin(images))\n",
    "    })\n",
    "    for camera_key, images in image_dicts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_0\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera_2\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n",
      "camera_1\n",
      "feat0 torch.Size([4, 96, 64, 64])\n",
      "feat1 torch.Size([4, 96, 32, 32])\n",
      "feat2 torch.Size([4, 96, 16, 16])\n",
      "feat3 torch.Size([4, 96, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "for camera_key, features in visual_features.items():\n",
    "    print(camera_key)\n",
    "    for key, feature in features.items():\n",
    "        print(key, feature.shape)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _voxelize(space, voxel_size):\n",
    "    X = torch.arange(space[0][0], space[0][1] + voxel_size[0]/8, voxel_size[0])\n",
    "    Y = torch.arange(space[1][0], space[1][1] + voxel_size[1]/8, voxel_size[1])\n",
    "    Z = torch.arange(space[2][0], space[2][1] + voxel_size[2]/8, voxel_size[2])\n",
    "    # print(X.shape, Y.shape, Z.shape)\n",
    "\n",
    "    grid_x, grid_y, grid_z = torch.meshgrid(X, Y, Z, indexing='ij')\n",
    "    return torch.stack([grid_x.flatten(), grid_y.flatten(), grid_z.flatten()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaces torch.Size([4, 2, 1, 3, 2]) torch.Size([4, 2, 1, 96])\n",
      "voxels torch.Size([4, 2, 1, 108, 3])\n",
      "------------------------------\n",
      "spaces torch.Size([4, 2, 6, 3, 2]) torch.Size([4, 2, 1, 96])\n",
      "voxels torch.Size([4, 2, 6, 64, 3])\n",
      "------------------------------\n",
      "spaces torch.Size([4, 2, 6, 3, 2]) torch.Size([4, 2, 1, 96])\n",
      "voxels torch.Size([4, 2, 6, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "n_class, k = 2, 6\n",
    "layers = nn.ModuleList([\n",
    "    cvmpca.VoxelMHA(\n",
    "        embed_dim=embed_dim, num_heads=12, attention_dropout=0.1, dropout=0.1,\n",
    "        cameras=cameras\n",
    "    ) for _ in range(len(swin.embed_dims))\n",
    "])\n",
    "cls_embedding = nn.Embedding(n_class, embed_dim)\n",
    "\n",
    "space = torch.tensor([[-11, 11], [0, 3], [-7, 7]])\n",
    "voxel_size = [2.7, 2.7, 2.7]\n",
    "\n",
    "def _original(indices, n_bins):\n",
    "    X, Y, Z = n_bins\n",
    "\n",
    "    x, y, z = (indices//Z)//Y, (indices//Z)%Y, indices%Z\n",
    "    print(x, y, z)\n",
    "    # return x, y, z\n",
    "    return torch.stack([x, y, z], dim=-1)\n",
    "\n",
    "query = cls_embedding.weight.unsqueeze(0).unsqueeze(-2).expand(B, -1, -1, -1)\n",
    "spaces = space.unsqueeze(0).unsqueeze(0).unsqueeze(0).expand(B, n_class, -1, -1, -1)\n",
    "for l_idx, layer in enumerate(layers):\n",
    "    print('spaces', spaces.shape, query.shape)\n",
    "    B, C, K, *_ = spaces.shape\n",
    "    next_voxel_size = np.array(voxel_size)/(3**l_idx)\n",
    "    \n",
    "    voxels = torch.stack([\n",
    "        torch.stack([\n",
    "            torch.stack([_voxelize(s, next_voxel_size) for s in sp])\n",
    "            for sp in space\n",
    "        ], dim=0)\n",
    "        for space in spaces\n",
    "    ], dim=0)\n",
    "    print(\"voxels\", voxels.shape)\n",
    "\n",
    "    B, C, K, N, _ = voxels.shape\n",
    "    \n",
    "    x, (space_idx, voxel_idx) = layer(\n",
    "        voxels + next_voxel_size/2, k,\n",
    "        query,\n",
    "        {key: list(features.values())[~l_idx] for key, features in visual_features.items()}\n",
    "    )  # voxels + next_voxel_size/2: center of voxels\n",
    "    # space_ids, voxel_ids = flatten_voxel_idx // n_bins, flatten_voxel_idx % n_bins\n",
    "\n",
    "    top_voxels = voxels.gather(\n",
    "        2, space_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, N, 3)\n",
    "    ).gather(\n",
    "        3, voxel_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, -1, 3)\n",
    "    ).squeeze(-2)\n",
    "    \n",
    "\n",
    "    spaces = torch.stack([\n",
    "        torch.stack([top_voxels[i], top_voxels[i] + next_voxel_size], dim=-1)\n",
    "        for i in range(top_voxels.shape[0])\n",
    "    ])\n",
    "    # spaces = torch.stack([top_voxels.unsqueeze(-1).unsqueeze(-1).expand(-1, C, K), top_voxels + next_voxel_size], dim=-1)\n",
    "    # print(top_voxels.shape, spaces.shape)\n",
    "    \n",
    "    if l_idx == 2: break\n",
    "    print('-'*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = cvmpca.CVMPCA(\n",
    "    train_loader.dataset.captures,\n",
    "    n_classes=len(train_loader.dataset.category_lookup),  # ignore background\n",
    "    spaces=[[-11, 11], [0, 3], [-7, 7]],\n",
    "    voxel_size=[2.7, 2.7, 2.7],\n",
    "    ratio=3,\n",
    ")\n",
    "# outputs = net(50, image_dicts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dicts, object_list = batch\n",
    "output_list, voxel_list = net(50, image_dicts)\n",
    "# net.cuda()\n",
    "# output_list, voxel_list = net(50, {key: item.cuda() for key, item in image_dicts.items()})\n",
    "# B, L, C, P, _ = cls_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.next_voxel_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 50, 3]) torch.Size([4, 1, 1, 3])\n",
      "torch.Size([4, 10, 50, 3]) torch.Size([4, 1, 1, 3])\n",
      "torch.Size([4, 10, 50, 3]) torch.Size([4, 1, 1, 3])\n",
      "torch.Size([4, 10, 50, 3]) torch.Size([4, 1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "(pred_ids, label_ids) = net._match((output_list, voxel_list), object_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 3, 50, 3]),\n",
       " torch.Size([4, 4, 3, 50, 3]),\n",
       " torch.Size([4, 4, 3, 50, 4]),\n",
       " torch.Size([4, 4, 3, 50, 3]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls = output_list[..., :net.n_classes].log_softmax(-1)\n",
    "pred_pos = output_list[..., net.n_classes:net.n_classes+3].sigmoid()\n",
    "pred_link = output_list[..., -net.n_cameras:]\n",
    "pred_voxel = voxel_list\n",
    "\n",
    "_device = pred_cls.device\n",
    "B, L, C, P, _ = pred_cls.shape\n",
    "G = object_list.position.shape[1]\n",
    "b_ids, l_ids, c_ids, p_idx, g_idx = torch.arange(B, device=_device), torch.arange(L, device=_device), torch.arange(C, device=_device), torch.arange(P, device=_device), torch.arange(G, device=_device)\n",
    "\n",
    "pred_cls.shape, pred_pos.shape, pred_link.shape, pred_voxel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 1, 1]), torch.Size([1, 4, 1, 1]), torch.Size([1, 1, 3, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ids[..., None, None, None].shape, l_ids[None, ..., None, None].shape, c_ids[None, None, ..., None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 3, 50, 3]),\n",
       " torch.Size([1, 1, 3, 1, 1]),\n",
       " torch.Size([4, 10, 3]),\n",
       " torch.Size([4, 4, 3, 10]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls.shape, c_ids[None, None, ..., None, None].shape, object_list.category.shape, pred_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1326, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_label = torch.zeros_like(pred_cls)\n",
    "class_label[\n",
    "    b_ids[..., None, None, None],\n",
    "    l_ids[None, ..., None, None],\n",
    "    c_ids[None, None, ..., None],\n",
    "] = 1\n",
    "class_label[..., 0] = 1 # every thing is a background\n",
    "# only a few have non-background labels\n",
    "class_label[\n",
    "    b_ids[..., None, None, None],\n",
    "    l_ids[None, ..., None, None],\n",
    "    c_ids[None, None, ..., None],\n",
    "    label_ids\n",
    "] = object_list.category.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "cross_entropy = F.cross_entropy(pred_cls.flatten(0, -2), class_label.flatten(0, -2).argmax(-1))\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 3, 50, 3]),\n",
       " torch.Size([4, 4, 3, 50, 3]),\n",
       " torch.Size([4, 3]),\n",
       " torch.Size([4, 10, 3]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pos.shape, voxel_list.shape, net.next_voxel_size.shape, object_list.position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 3, 10, 3]), torch.Size([1, 4, 1, 1, 3]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pos[\n",
    "    b_ids[..., None, None, None],\n",
    "    l_ids[None, ..., None, None],\n",
    "    c_ids[None, None, ..., None],\n",
    "    label_ids\n",
    "].shape, net.next_voxel_size.unsqueeze(1).unsqueeze(1).unsqueeze(0).shape\n",
    "\n",
    "# mse_loss = F.mse_loss(\n",
    "#     pred_pos[\n",
    "#         b_ids[..., None, None, None],\n",
    "#         l_ids[None, ..., None, None],\n",
    "#         c_ids[None, None, ..., None],\n",
    "#         label_ids\n",
    "#     ] * net.next_voxel_size.unsqueeze(0).unsqueeze(1).unsqueeze(1) + voxel_list[\n",
    "#         b_ids[..., None, None, None],\n",
    "#         l_ids[None, ..., None, None],\n",
    "#         c_ids[None, None, ..., None],\n",
    "#         label_ids\n",
    "#     ],\n",
    "#     object_list.position.unsqueeze(1).unsqueeze(1)\n",
    "# )\n",
    "# mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 3, 50, 4]), torch.Size([4, 10, 4]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_link.shape, object_list.los.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeonardNgo\\AppData\\Local\\Temp\\ipykernel_93524\\3259707870.py:1: UserWarning: Using a target size (torch.Size([4, 1, 1, 10, 4])) that is different to the input size (torch.Size([4, 4, 3, 10, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  los_link_loss = F.mse_loss(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8090, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "los_link_loss = F.mse_loss(\n",
    "    pred_link[\n",
    "        b_ids[..., None, None, None],\n",
    "        l_ids[None, ..., None, None],\n",
    "        c_ids[None, None, ..., None],\n",
    "        label_ids\n",
    "    ],\n",
    "    object_list.los.unsqueeze(1).unsqueeze(1)\n",
    ")\n",
    "los_link_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\LeonardNgo\\Projects\\CV-MPCA\\src\\cvmpca.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('spaces', torch.tensor(spaces))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0 | swin    | Swin                  | 27.6 M\n",
      "1 | fpn     | FeaturePyramidNetwork | 470 K \n",
      "2 | decoder | ModuleList            | 522 K \n",
      "3 | heads   | ModuleList            | 38.7 K\n",
      "4 | query   | Embedding             | 288   \n",
      "--------------------------------------------------\n",
      "28.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.6 M    Total params\n",
      "114.454   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817363b0e00441f999997596fd7f1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=100, precision='16-mixed',\n",
    "    gradient_clip_val=35,\n",
    "    log_every_n_steps=20,\n",
    "    # accelerator=\"cpu\"\n",
    "    # profiler=\"simple\",\n",
    ")\n",
    "net = cvmpca.CVMPCA(\n",
    "    train_loader.dataset.captures,\n",
    "    n_classes=len(train_loader.dataset.category_lookup), \n",
    "    spaces=torch.tensor([[-11, 11], [0, 3], [-7, 7]]),\n",
    "    voxel_size=[2.7, 2.7, 2.7],\n",
    "    ratio=3,\n",
    ")\n",
    "# loss_func = SetCriterion(\n",
    "#     num_ue=50, num_sbs=2, num_class=1,\n",
    "#     num_layers=3, pc_range=[[-5, 5], [0, 3], [-5, 5]]\n",
    "# )\n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, C, P, G, D = 2, 4, 3, 5, 3, 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
