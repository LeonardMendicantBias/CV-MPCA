{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pysolotools.consumers import Solo\n",
    "from pysolotools.converters.solo2coco import SOLO2COCOConverter\n",
    "from pysolotools.core.models import KeypointAnnotationDefinition, RGBCameraCapture\n",
    "from pysolotools.core.models import BoundingBox2DLabel, BoundingBox2DAnnotation\n",
    "from pysolotools.core.models import BoundingBox3DLabel, BoundingBox3DAnnotation\n",
    "from pysolotools.core.models import Frame, Capture\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.io import savemat, loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformer\n",
    "import reader\n",
    "import network\n",
    "# from my_trainer import LitCVMPCA, HungarianMatcher, SetCriterion\n",
    "\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_folder = 'D:/Unity/dataset/solo'\n",
    "train_loader = reader.UnityDataset.from_unity_to_loader(train_folder, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.49%\r"
     ]
    }
   ],
   "source": [
    "backbone = transformer.Swin(is_trainable=False)\n",
    "backbone.cuda()\n",
    "backbone.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(train_loader.dataset):\n",
    "        path_dict, image_dict, _ = sample\n",
    "        image_dict = {key: item[None, ...].cuda() for key, item in image_dict.items()}\n",
    "        for key, image in image_dict.items():  # all images in \n",
    "            path = path_dict[key]\n",
    "            _path = f'{path[:path.rfind(\"/\")]}/{key}.mat'\n",
    "            outputs = backbone(image)\n",
    "            output_dict = {str(i): output[0].cpu().numpy() for i, output in enumerate(outputs)}\n",
    "            savemat(_path, output_dict)\n",
    "\n",
    "        print(f'{i/len(train_loader.dataset)*100:.2f}%', end='\\r')\n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 99.50%"
     ]
    }
   ],
   "source": [
    "train_loader = reader.UnityMatDataset.from_unity_to_loader(train_folder, batch_size=4)\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, targets = batch\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 99.50%"
     ]
    }
   ],
   "source": [
    "for sample in train_loader.dataset:\n",
    "    path_dict, images, targets = sample\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, W, C = 4, 8, 8, 16\n",
    "inp = torch.rand((B, C, H, W))\n",
    "pixel_coor = torch.rand((4, 1, 2))\n",
    "\n",
    "sampled_inp = F.grid_sample(inp, pixel_coor[None, ...], align_corners=True)\n",
    "sampled_inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvmpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
